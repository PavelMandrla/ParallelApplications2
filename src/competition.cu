// includes, cuda

#include <cstdint>
#include <climits>
#include <cuda_runtime.h>
#include <helper_cuda.h>

#include <cudaDefs.h>
#include <imageManager.h>
#include <cfloat>

#include <arrayUtils.cuh>
#include <imageUtils.cuh>
#include <benchmark.h>

#define TPB_1D 8						// ThreadsPerBlock in one dimension
#define TPB_2D TPB_1D*TPB_1D			// ThreadsPerBlock = TPB_1D*TPB_1D (2D block)
#define TPB_REDUCTION 512				// ThreadsPerBlock (1D block)

cudaError_t error = cudaSuccess;
cudaDeviceProp deviceProp = cudaDeviceProp();

using DT = uint32_t;

struct alignas(8) ResultType
{
	float fitness;
	uint32_t idx;
	
	__host__ __device__ ResultType& operator=(ResultType&&) = default;				//Forcing a move assignment operator to be generated by the compiler

	__host__ __device__ volatile ResultType& operator=(volatile const ResultType& other) volatile
	{
		fitness = other.fitness;
		idx = other.idx;
		return *this;
	}
};

#pragma region KERNELS

/// <summary>
/// Simple reduction with use of a single 1D thread block
/// </summary>
/// <param name="data">Data to be reduced.</param>
/// <param name="length">Data length.</param>
/// <returns>void ... the result is stored in data[0].</returns>
__global__ void getBest(ResultType* data, const uint32_t length)
{
	__shared__ ResultType sData[TPB_REDUCTION];

	uint32_t tid = threadIdx.x;
	const uint32_t offset = blockDim.x;

	sData[tid] = { FLT_MAX , tid };								//Initial fill of the shared memory

	if (tid >= length) return;

	sData[tid] = data[tid];

	uint32_t nextId = tid + offset;
	ResultType* ptr = &data[nextId];							//Pointer to global mem;

	while (nextId < length)										//Compare rest of data from the global memory
	{
		if (ptr->fitness < sData[tid].fitness)
		{
			sData[tid] = *ptr;
		}
		ptr += offset;
		nextId += offset;
	}
	__syncthreads();											//Start reduction from now

	if constexpr (TPB_REDUCTION >= 128)
	{
#pragma unroll
		for (uint32_t s = (TPB_REDUCTION >> 1); s > 32; s >>= 1)	//This can be UNROLLED when the TPB is fixed for the application
		{
			if (tid < s)
			{
				if (sData[tid + s].fitness < sData[tid].fitness)
				{
					sData[tid] = sData[tid + s];
				}
			}
			__syncthreads();
		}
	}

	if (tid < 32)												//Only one warp is active here, no sync is needed.
	{
		volatile ResultType* vsData = sData;
		if constexpr (TPB_REDUCTION >= 64) vsData[tid] = (vsData[tid].fitness < vsData[tid + 32].fitness) ? vsData[tid] : vsData[tid + 32];
		if constexpr (TPB_REDUCTION >= 32) vsData[tid] = (vsData[tid].fitness < vsData[tid + 16].fitness) ? vsData[tid] : vsData[tid + 16];
		if constexpr (TPB_REDUCTION >= 16) vsData[tid] = (vsData[tid].fitness < vsData[tid + 8].fitness) ? vsData[tid] : vsData[tid + 8];
		if constexpr (TPB_REDUCTION >= 8) vsData[tid] = (vsData[tid].fitness < vsData[tid + 4].fitness) ? vsData[tid] : vsData[tid + 4];
		if constexpr (TPB_REDUCTION >= 4) vsData[tid] = (vsData[tid].fitness < vsData[tid + 2].fitness) ? vsData[tid] : vsData[tid + 2];
		if constexpr (TPB_REDUCTION >= 2) vsData[tid] = (vsData[tid].fitness < vsData[tid + 1].fitness) ? vsData[tid] : vsData[tid + 1];
	}


	if (tid == 0)												//The zero thread saves the result into Global mem
	{
		data[0] = sData[0];
	}
}

/// <summary>
/// Every THREAD of 2D block [TPB_1DxTPB_1D] computes one final fitness value for a single pixel of the reference image. One corner of the query image is "virtually" attached to this pixel position. 
/// A SINGLE THREAD compares the query image with the given region of the reference image. 
/// </summary>
/// <param name="ref">Reference data.</param>
/// <param name="rWidth">Width of the reference data.</param>
/// <param name="rHeight">Height of the reference data.</param>
/// <param name="query">Query data.</param>
/// <param name="qWidth">Width of the query data.</param>
/// <param name="qHeight">Height of the query data.</param>
/// <param name="blockResults">Each block of the grid will store exactly one value into the global memory. Them, this data is reduced in another kernel to find the best value/solution.</param>
/// <returns></returns>
__global__ void find(const DT* __restrict__ ref, const uint32_t rWidth, const uint32_t rHeight,
	const DT* __restrict__ query, const uint32_t qWidth, const uint32_t qHeight,
	ResultType* __restrict__ blockResults)
{
	uint32_t tid = threadIdx.x + threadIdx.y * blockDim.x;
	uint32_t rx = blockIdx.x * blockDim.x + threadIdx.x;
	uint32_t ry = blockIdx.y * blockDim.y + threadIdx.y;
	uint32_t qx, qy;
	const DT* r = nullptr;
	const DT* q = nullptr;

	__shared__ ResultType sData[TPB_2D];
	sData[tid] = { FLT_MAX, ry * rWidth + rx };

	//EUCLIDEAN DISTANCE
	/*
	if ((ry <= rHeight - qHeight) && (rx <= rWidth - qWidth))
	{
		r = &ref[ry * rWidth + rx];												//Pointer to starting ROW position in the reference image.
		q = &query[0];															//Pointer to starting ROW position in the query image.

		sData[tid].fitness = 0.0f;
		for (qy = 0; qy < qHeight; qy++)										//Each thread will process the whole query image
		{
			for (qx = 0; qx < qWidth; qx++)										//Each thread will process the whole query image
			{

				sData[tid].fitness += (r[qx] - q[qx]) * (r[qx] - q[qx]);		//Cummulate the value
			}
			r += rWidth;														//Move one row down in the reference image.
			q += qWidth;														//Move one row down in the query image.
		}
	}
	*/

	//EQUALITY CHECK ONLY
	bool equality = true;
	if ((ry <= rHeight - qHeight) && (rx <= rWidth - qWidth))
	{
		r = &ref[ry * rWidth + rx];												//Pointer to starting ROW position in the reference image.
		q = &query[0];															//Pointer to starting ROW position in the query image.

		for (qy = 0; equality && (qy < qHeight); qy++)							//Each thread will process the whole query image		
		{
			for (qx = 0; equality && (qx < qWidth); qx++)						//Each thread will process the whole query image
			{
				equality = (r[qx] == q[qx]);
			}
			r += rWidth;														//Move one row down in the reference image.
			q += qWidth;														//Move one row down in the query image.
		}
		if (equality)
			sData[tid].fitness = 0.0f;
	}

	__syncthreads();										//The parallel reduction will start here, all WARPS has to finish previous instructions.

	if constexpr (TPB_2D >= 128)
	{
		#pragma unroll
		for (uint32_t s = (TPB_2D >> 1); s > 32; s >>= 1)		//This can be UNROLLED when the TPB is fixed for the application
		{
			if (tid < s)
			{
				if (sData[tid + s].fitness < sData[tid].fitness)
				{
					sData[tid] = sData[tid + s];
				}
			}
			__syncthreads();
		}
	}

	if (tid < 32)											//Only one warm is active here, no sync is needed.
	{
		volatile ResultType* vsData = sData;
		if constexpr (TPB_2D >= 64) vsData[tid] = (vsData[tid].fitness < vsData[tid + 32].fitness) ? vsData[tid] : vsData[tid + 32];
		if constexpr (TPB_2D >= 32) vsData[tid] = (vsData[tid].fitness < vsData[tid + 16].fitness) ? vsData[tid] : vsData[tid + 16];
		if constexpr (TPB_2D >= 16) vsData[tid] = (vsData[tid].fitness < vsData[tid + 8].fitness) ? vsData[tid] : vsData[tid + 8];
		if constexpr (TPB_2D >= 8)	vsData[tid] = (vsData[tid].fitness < vsData[tid + 4].fitness) ? vsData[tid] : vsData[tid + 4];
		if constexpr (TPB_2D >= 4)	vsData[tid] = (vsData[tid].fitness < vsData[tid + 2].fitness) ? vsData[tid] : vsData[tid + 2];
		if constexpr (TPB_2D >= 2)	vsData[tid] = (vsData[tid].fitness < vsData[tid + 1].fitness) ? vsData[tid] : vsData[tid + 1];
	}

	if (tid == 0)											//0-th thread stores the final BEST result for a given block
	{
		blockResults[blockIdx.y * gridDim.x + blockIdx.x] = sData[0];
		//printf("%d %f\n", blockIdx.y * gridDim.x + blockIdx.x, sData[0].fitness);
	}

}

/// <summary>
/// Every THREAD of 2D block [TPB_1DxTPB_1D] computes one final fitness value for a single pixel of the reference image. One corner of the query image is "virtually" attached to this pixel position. 
/// A SINGLE THREAD compares the query image with the given region of the reference image. 
/// </summary>
/// <param name="ref">Reference data stored in a TextureObject.</param>
/// <param name="rWidth">Width of the reference data.</param>
/// <param name="rHeight">Height of the reference data.</param>
/// <param name="query">Query data stored in a TextureObject.</param>
/// <param name="qWidth">Width of the query data.</param>
/// <param name="qHeight">Height of the query data.</param>
/// <param name="blockResults">Each block of the grid will store exactly one value into the global memory. Them, this data is reduced in another kernel to find the best value/solution.</param>
/// <returns></returns>
__global__ void findTex(const cudaTextureObject_t ref, const uint32_t rWidth, const uint32_t rHeight,
	const cudaTextureObject_t query, const uint32_t qWidth, const uint32_t qHeight,
	ResultType* __restrict__ blockResults)
{
	uint32_t tid = threadIdx.x + threadIdx.y * blockDim.x;
	uint32_t rx = blockIdx.x * blockDim.x + threadIdx.x;
	uint32_t ry = blockIdx.y * blockDim.y + threadIdx.y;
	uint32_t qx, qy;
	uint32_t r, q;

	__shared__ ResultType sData[TPB_2D];
	sData[tid] = { FLT_MAX, ry * rWidth + rx };

	//EUCLIDEAN DISTANCE
	/*
	if ((ry <= rHeight - qHeight) && (rx <= rWidth - qWidth))
	{
		sData[tid].fitness = 0.0f;
		for (qy = 0; qy < qHeight; qy++, ry++)						//Each thread will process the whole query image		
		{
			for (qx = 0; qx < qWidth; qx++, rx++)						//Each thread will process the whole query image
			{
				r = tex2D<uint32_t>(ref, rx, ry);
				q = tex2D<uint32_t>(query, qx, qy);
				sData[tid].fitness += (r-q) * (r-q);								//Cummulate the value
			}
			rx = blockIdx.x * blockDim.x + threadIdx.x;
		}
	}
	*/

	//EQUALITY CHECK ONLY
	bool equality = true;
	if ((ry <= rHeight - qHeight) && (rx <= rWidth - qWidth))
	{
		for (qy = 0; equality && (qy < qHeight); qy++, ry++)						//Each thread will process the whole query image		
		{
			for (qx = 0; equality && (qx < qWidth); qx++, rx++)						//Each thread will process the whole query image
			{
				r = tex2D<uint32_t>(ref, rx, ry);
				q = tex2D<uint32_t>(query, qx, qy);
				equality = (r == q);
			}
			rx = blockIdx.x * blockDim.x + threadIdx.x;
		}
		if (equality)
			sData[tid].fitness = 0.0f;
	}

	__syncthreads();																//The parallel reduction will start here, all WARPS has to finish previous instructions.

	if constexpr (TPB_2D >= 128)
	{
#pragma unroll
		for (uint32_t s = (TPB_2D >> 1); s > 32; s >>= 1)							//This can be UNROLLED when the TPB is fixed for the application
		{
			if (tid < s)
			{
				if (sData[tid + s].fitness < sData[tid].fitness)
				{
					sData[tid] = sData[tid + s];
				}
			}
			__syncthreads();
		}
	}

	if (tid < 32)											//Only one warm is active here, no sync is needed.
	{
		volatile ResultType* vsData = sData;
		if constexpr (TPB_2D >= 64) vsData[tid] = (vsData[tid].fitness < vsData[tid + 32].fitness) ? vsData[tid] : vsData[tid + 32];
		if constexpr (TPB_2D >= 32) vsData[tid] = (vsData[tid].fitness < vsData[tid + 16].fitness) ? vsData[tid] : vsData[tid + 16];
		if constexpr (TPB_2D >= 16) vsData[tid] = (vsData[tid].fitness < vsData[tid + 8].fitness) ? vsData[tid] : vsData[tid + 8];
		if constexpr (TPB_2D >= 8)	vsData[tid] = (vsData[tid].fitness < vsData[tid + 4].fitness) ? vsData[tid] : vsData[tid + 4];
		if constexpr (TPB_2D >= 4)	vsData[tid] = (vsData[tid].fitness < vsData[tid + 2].fitness) ? vsData[tid] : vsData[tid + 2];
		if constexpr (TPB_2D >= 2)	vsData[tid] = (vsData[tid].fitness < vsData[tid + 1].fitness) ? vsData[tid] : vsData[tid + 1];
	}

	if (tid == 0)											//0-th thread stores the final BEST result for a given block
	{
		blockResults[blockIdx.y * gridDim.x + blockIdx.x] = sData[0];
		//printf("%d %f\n", blockIdx.y * gridDim.x + blockIdx.x, sData[0].fitness);
	}

}

#pragma endregion KERNELS

#pragma region TEXTURES

__host__ TextureInfo createTextureObjectFrom2DArray(const ImageInfo<DT>& ii)
{
	TextureInfo ti;

	// Size info
	ti.size = { ii.width, ii.height, 1 };

	//Texture Data settings
	ti.texChannelDesc = cudaCreateChannelDesc<uint32_t>();  // cudaCreateChannelDesc(32, 0, 0, 0, cudaChannelFormatKindUnsigned);
	checkCudaErrors(cudaMallocArray(&ti.texArrayData, &ti.texChannelDesc, ii.width, ii.height));
	checkCudaErrors(cudaMemcpyToArray(ti.texArrayData, 0, 0, ii.dPtr, ii.pitch * ii.height, cudaMemcpyDeviceToDevice));

	// Specify texture resource
	ti.resDesc.resType = cudaResourceTypeArray;
	ti.resDesc.res.array.array = ti.texArrayData;

	// Specify texture object parameters
	ti.texDesc.addressMode[0] = cudaAddressModeClamp;
	ti.texDesc.addressMode[1] = cudaAddressModeClamp;
	ti.texDesc.filterMode = cudaFilterModePoint;
	ti.texDesc.readMode = cudaReadModeElementType;
	ti.texDesc.normalizedCoords = false;

	// Create texture object
	checkCudaErrors(cudaCreateTextureObject(&ti.texObj, &ti.resDesc, &ti.texDesc, NULL));

	return ti;
}

#pragma endregion TEXTURES

#pragma region PROCESSING

template<bool USE_TEXTURES=false>
void processData(const ImageInfo<DT>& ref, const ImageInfo<DT>& query)
{
	float gpuTime = 0.0;

	//How many block of the size of [16x16] will process the reference image? 
	//Too much to manage. That's we use a 1D grid of [16x16] blocks that will move down the image.
	//This we need (((ref.width - query.width + 1) + 16 - 1)/16) blocks!!!
	dim3 block{ TPB_1D , TPB_1D ,1 };
	dim3 grid{ ((ref.width - query.width + 1) + TPB_1D - 1) / TPB_1D,
			  ((ref.height - query.height + 1) + TPB_1D - 1) / TPB_1D,
			  1 };

	ResultType* dBlockResults = nullptr;
	auto dBlockResultsLength = grid.x * grid.y;
	checkCudaErrors(cudaMalloc(&dBlockResults, dBlockResultsLength * sizeof(ResultType)));

	//1. Try to compute all possible matches.
	if constexpr (USE_TEXTURES == false) {
		gpuTime = GPUTIME(1, find << <grid, block >> > (ref.dPtr, ref.width, ref.height, query.dPtr, query.width, query.height, dBlockResults));
		printf("\x1B[93m[GPU time] %s: %f ms\033[0m\n", "find", gpuTime);
	} else {
		TextureInfo tiRef = createTextureObjectFrom2DArray(ref);
		TextureInfo tiQuery = createTextureObjectFrom2DArray(query);

		gpuTime = GPUTIME(1, findTex <<<grid, block >>> (tiRef.texObj, tiRef.size.width, tiRef.size.height, tiQuery.texObj, tiQuery.size.width, tiQuery.size.height, dBlockResults));
		printf("\x1B[93m[GPU time] %s: %f ms\033[0m\n", "findTex", gpuTime);

		if (tiRef.texObj) checkCudaErrors(cudaDestroyTextureObject(tiRef.texObj));
		if (tiRef.texArrayData)	checkCudaErrors(cudaFreeArray(tiRef.texArrayData));
		if (tiQuery.texObj) checkCudaErrors(cudaDestroyTextureObject(tiQuery.texObj));
		if (tiQuery.texArrayData) checkCudaErrors(cudaFreeArray(tiQuery.texArrayData));
	}


	//2. Search for the best match
	block = { TPB_REDUCTION ,1,1 };
	grid = { 1, 1, 1 };
	gpuTime = GPUTIME(1, getBest << <grid, block >> > (dBlockResults, dBlockResultsLength));
	printf("\x1B[93m[GPU time] %s: %f ms\033[0m\n", "getBest", gpuTime);


	ResultType bestBlockResult;
	checkCudaErrors(cudaMemcpy(&bestBlockResult, dBlockResults, sizeof(ResultType), cudaMemcpyKind::cudaMemcpyDeviceToHost));

	printf("Best fitness value: %f\n", bestBlockResult.fitness);
	printf("Winner index: %u\n", bestBlockResult.idx);
	printf("Winner's LEFT-TOP CORNER X: %u\n", bestBlockResult.idx % ref.width);
	printf("Winner's LEFT-TOP CORNER Y: %u\n", ref.height - (bestBlockResult.idx / ref.width) - query.height);

	if (dBlockResults) cudaFree(dBlockResults);
}

#pragma endregion PROCESSING

int main(int argc, char* argv[])
{
	initializeCUDA(deviceProp);
	FreeImage_Initialise();

	ImageInfo<DT> ref;
	ImageInfo<DT> query;

	prepareData<false>("../../images/reference.tif", ref);
	prepareData<false>("../../images/query.tif", query);
	
	processData<false>(ref, query);

	FreeImage_DeInitialise();
	if (ref.dPtr) cudaFree(ref.dPtr);
	if (query.dPtr) cudaFree(query.dPtr);
}
